""" This module demonstrates several of the ways to clean data
    using Pandas.
"""
__version__ = '0.3'
__author__ = 'Cesar Cesarotti'

import pandas as pd

data = pd.read_csv('../data/movie_metadata.csv')

data.info()
data.head()

# Normalize data types
# Sometimes, especially when you're reading in a CSV with a bunch of numbers,
# some of the numbers will read in as strings instead of numeric values,
# or vice versa. Here's a way you can fix your data types:

data = pd.read_csv('../data/movie_metadata.csv', dtype={'title_year': str})
data.info()

# dtype takes a dictionary, so you can do multiple columns at once:

# Keep in mind that this data reads the CSV from disk again...

# Dealing with missing data
# One of the most common problems is missing data.
# There are a couple of ways to deal with missing data:

# Add Default Values:
data.country = data.country.fillna('')

data.duration = data.duration.fillna(data.duration.mean())

# Remove incomplete rows:

# Dropping all rows with ANY NA values is easy:
#data.dropna()

# We can also drop only the rows that have all NA values:
#data.dropna(how='all')

# We can also put a limitation on how many non-null values need to be in a
# row in order to keep it (in this example, the data needs to have at
# least 5 non-null values):
#data.dropna(thresh=5)

# We can also decide to only drop rows that have no data in a
# particular column:

#data.dropna(subset=['title_year'])

#The subset parameter can take a list of column names.

# Deal with error-prone columns

# We can apply the same kind of criteria to our columns. We just need to use
# the parameter axis=1 in our code. That means to operate on columns, not
# rows. (We could have used axis=0 in our row examples, but it is 0 by
# default if you don't enter anything.) Examples:

#data.dropna(axis=1, how='all') # Drops columns that have all NA values

#data.dropna(axis=1, how='any') # Drops columns that have ANY NA values

# The same threshold and subset parameters from above apply as well.

# Change casing
# Columns with user-provided data are ripe for erros. People make typos,
# leave their caps lock on (or off), and add erroneous extra spaces.

# To change all our movie titles to uppercase:

data['movie_title'] = data['movie_title'].str.upper()

# Similarly, to get rid of trailing whitespace:

data['movie_title'] = data['movie_title'].str.strip()

# Rename columns

# Finally, if your data was generated by a computer program,
# it probably has some computer-generated column names, too.
# Those can be hard to read and understand while working,
# so if you want to rename a column to something more user-friendly,
# you can do it like this:

# To rename 'title_year' to 'release_date' and
# 'movie_facebook_likes' to simply 'facebook_likes':

data = data.rename(columns={'title_year':'release_date', 'movie_facebook_likes':'facebook_likes'})

# Save your results!

# data.to_csv('cleanmovies.csv' encoding='utf-8')

# Let's look at some other data!

# pdata = pd.read_csv('../data/patient_heart_rate.csv')
# print(pdata)

# There aren't any column headers! Let's add some:

# column_names= ["Id", "Name", "Age", "Weight",'m0006','m0612','m1218','f0006','f0612','f1218']
# pdata = pd.read_csv("../data/patient_heart_rate.csv", names = column_names)
# print(pdata)

# Name is a combo of first and last name. Let's make that two columns:

# pdata[['Firstname','Lastname']] = pdata['Name'].str.split(expand=True)
# pdata = pdata.drop('Name', axis=1)
# print(pdata)

# Some of the weights are in lbs, others in kg. Let's go metric!

def lbconvert(weight):
    ''' Converts weights in pounds to weights in kg.
    '''
    weight = str(weight)
    if weight[-3:] == 'lbs':
        value = weight[:-3]
        fval = float(value)
        fval = fval/2.2
        return str(int(fval)) + "kgs"
    return weight

# pdata['Weight']=pdata['Weight'].apply(lbconvert, convert_dtype=False)
# print(pdata['Weight'])

# Turns out they got the wrong mouse,
# so let's change all the Mickeys to Minnies:

# pdata.loc[ pdata['Firstname'] == 'Mick√©y', 'Firstname'] = 'Minnie'
